In case I forget, this project is to explore the use of Vision Language Action models (VLAs) from openvla by Moo Jin Kim as policies for the ALOHA set up by Tony Z. Zhao.
This is for preparation of the low cost 3D printed Arm we're working on at Google Developer Groups on Campus Robotics Track

Tasks include:
Picking up a box
Sorting boxes
Transfer of objects from one manipulator to another

![image](https://github.com/user-attachments/assets/a7b0e51f-9703-41d7-835a-f2ebd81f1823)



On the GDG repo, we'll drop code and materials relating to the physical arm, and simulations with the panda robot (similar single set up)

These are just my thoughts so I don't forget :)

Make sure to cite all works!!!
